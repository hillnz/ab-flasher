#!/usr/bin/env python3

import asyncio
import errno
import hashlib
import logging
import os
import re
import shutil
import sys
import zlib
from asyncio import subprocess
from contextlib import asynccontextmanager
from distutils.version import LooseVersion
from functools import wraps
from glob import glob
from itertools import takewhile
from os import makedirs
from shutil import rmtree
from tempfile import mkdtemp
from typing import Callable, Coroutine, List, OrderedDict, Tuple

import aiofiles
import aiohttp
import typer
from aiohttp.client_exceptions import ClientResponseError
import shlex

log = logging.getLogger('ab-flasher')

app = typer.Typer(add_completion=False, context_settings={ 'auto_envvar_prefix': 'AB_' })

def die(message: str, code: int=1):
    if message:
        log.critical(message)
    sys.exit(code)

def run_sync(f):
    @wraps(f)
    def wrap(*args, **kwargs):
        return asyncio.run(f(*args, **kwargs))
    return wrap
    
# Unknown if optimal, this is what shutil uses
COPY_BUFSIZE = 1024 * 1024

RE_OS_FILES = re.compile('(' + ')|('.join([
    r'/kernel[^/]*\.img$',
    r'/init[^/]*$',
    r'/cmdline.txt$',
    r'\.dtbo?$'
]) + ')')

async def pipeline(*stage_funcs):
    # Build chain, passing lower stages to higher ones
    lower = None
    stages = []
    for stage_func in reversed(stage_funcs):
        if lower is None:
            lower = stage_func()
        else:
            lower = stage_func(lower.asend)
        stages.append(lower)
    stages.reverse()
    
    producer = stages.pop(0)
    # Start the generators (producer isn't one, it just sends to the lower layers)
    for stage in stages:
        await stage.asend(None)
    # When this completes it means all has been sent to the lower layers
    await producer
    for stage in stages:
        await stage.aclose()


async def get_mounts(host) -> List[dict]:
    proc_mounts = os.path.join(host, 'proc/mounts')

    try:
        async with aiofiles.open(proc_mounts, 'r') as f:
            mounts_file = (await f.read()).strip()
    except FileNotFoundError:
        die('Couldn\'t find /proc/mounts (not Linux or not mounted?)')
    except:
        die('Couldn\'t read mounts (try running as root?)')
        
    mounts = []
    for mount in mounts_file.split('\n'):
        device, path, fs, options, _ = tuple(mount.split(maxsplit=4))
        options = options.split(',')
        mounts.append({ 'device': device, 'path': path, 'fs': fs, 'options': options })

    return mounts

async def run(program, *args, check=True, stdout=subprocess.PIPE, **kwargs):
    log.debug(f'Running: {program} {" ".join([ shlex.quote(a) for a in args ])}')
    proc = await asyncio.create_subprocess_exec(program, *args, stdout=stdout, **kwargs)
    out, _ = await proc.communicate()
    if check and proc.returncode != 0:
        raise Exception(f'{program} has returncode {proc.returncode}')
    if stdout == subprocess.PIPE:
        return out.decode().strip()

@asynccontextmanager
async def mount(host, device, mode='rw'):
    mounts = await get_mounts(host)
    mount = next(( m for m in mounts if m['device'] == device and mode in m['options'] ), None)
    if mount:
        yield mount['path']
        return
    
    log.debug(f'{device} is not mounted yet')

    mount_point = mkdtemp()
    try:
        returncode = await (await asyncio.create_subprocess_exec('mount', '-o', 'rw', device, mount_point)).wait()
        if returncode != 0:
            raise Exception(f'mount had returncode {returncode}')
        yield mount_point
    finally:
        await (await asyncio.create_subprocess_exec('umount', mount_point)).wait()

async def get_device_for_path(path='/'):
    return await run('findmnt', '-n', '-o', 'SOURCE', path)

async def get_parent_device(dev_path):
    return '/dev/' + (await run('lsblk', '-no', 'pkname', dev_path))

async def list_partitions(dev_path):
    output = await run('fdisk', '-l', '-o', 'Device', dev_path)
    # Take from end until we see the 'Device' header
    lines = iter(reversed(output.splitlines()))
    return list(reversed(list(takewhile(lambda l: l != 'Device', lines))))

async def get_http_bytes(url, send_download_bytes: Callable[[bytes], Coroutine]):
    log.debug('get_http_bytes')
    bytes = 0
    async with aiohttp.request('GET', url, raise_for_status=True) as resp:
        content_length = resp.content_length
        log.info(f'Expecting to download {content_length} bytes')
        async for chunk in resp.content.iter_chunked(COPY_BUFSIZE):
            bytes += len(chunk)
            await send_download_bytes(chunk)
    log.debug(f'Downloaded {bytes} bytes')


async def check_version_file(host, version_file, expected_version, dry_run: bool):
    abs_v_file = os.path.join(host, version_file)
    log.debug(f'Expecting version file at {abs_v_file}')
    try:
        async with aiofiles.open(abs_v_file, 'r') as f:
            current_version = (await f.read()).strip()
    except FileNotFoundError:
        log.debug('Current partition has no version, assuming 0')
        current_version = '0'
        if dry_run:
            log.warning('Dry run - would create version file')
        else:
            log.debug('Creating current version file')
            makedirs(os.path.dirname(abs_v_file), exist_ok=True)
            async with aiofiles.open(abs_v_file, 'w') as f:
                await f.write(current_version)
    except IOError as e:
        if e[0] == errno.EPERM:
            die('You don\'t have permission to access the version file (try running as root?)')
        else:
            raise
    
    log.debug(f'current_version is {current_version}. expected_version is {expected_version}.')
    try:
        if LooseVersion(current_version) >= LooseVersion(expected_version):
            log.info(f'No upgrade is required ({current_version} >= {expected_version})')
            return False
    except TypeError:
        log.warning('Version comparison failed, assuming upgrade is required')
    
    return True


async def get_partition(host, part_dev):
    proc_parts = os.path.join(host, '/proc/partitions')
    async with aiofiles.open(proc_parts, 'r') as f:
        await f.readline() # header
        await f.readline() # blank
        while True:
            item = tuple((await f.readline()).strip().split())
            if not item:
                break
            _, _, blocks, name = item
            if part_dev.endswith(name):
                return name, blocks * 1024
    return None, None

# TODO can decompress/write run in a thread. Any real performance gains?
async def decompress(send_raw_bytes):
    log.debug('decompress')
    in_bytes = out_bytes = 0
    try:
        gz = zlib.decompressobj(zlib.MAX_WBITS|32)
        
        while True:
            try:
                gz_chunk = yield
                in_bytes += len(gz_chunk)
                raw_chunk = gz.decompress(gz_chunk)
                out_bytes += len(raw_chunk)
                await send_raw_bytes(raw_chunk)
            except StopAsyncIteration:
                raise # Hack - disable unreachable code warning
    finally:
        log.debug(f'Decompressed {in_bytes} bytes to {out_bytes} bytes')


async def write_to_partition(partition, max_length=0):
    log.debug('write_to_partition')
    if max_length <= 0:
        log.warning('partition max_length isn\'t set so it won\'t be checked')
    written_bytes = 0
    try:
        async with aiofiles.open(partition, 'wb') as f:
            while True:
                chunk = yield
                if chunk is not None:
                    if (max_length > 0) and (written_bytes + len(chunk) > max_length):
                        # TODO more specific exception
                        raise Exception('length would exceed')
                    await f.write(chunk)
                    written_bytes += len(chunk)
    finally:
        log.info(f'Wrote {written_bytes} bytes')


async def untar(out_dir):
    # There isn't an obvious way to use tarfile from a stream, so we use `tar`
    tmp = mkdtemp()
    pipe_path = os.path.join(tmp, 'tar_pipe')
    try:
        os.mkfifo(pipe_path, mode=0o600)
        
        tar_proc = await asyncio.create_subprocess_exec('tar', 'xzf', '-' ,'-C', out_dir, stdin=subprocess.PIPE)
        try:
            while True:
                try:
                    tar_chunk = yield
                    tar_proc.stdin.write(tar_chunk)
                    await tar_proc.stdin.drain()
                except StopAsyncIteration:
                    raise # Hack - disable unreachable code warning
        finally:
            tar_proc.stdin.write_eof()
            await tar_proc.stdin.drain()
            if await tar_proc.wait() != 0:
                die('Untar of boot files failed')
    finally:
        os.unlink(pipe_path)
        os.rmdir(tmp)


async def digest_file(hasher, file_path: str, length: int = 0):
    async with aiofiles.open(file_path, 'rb') as f:
        bytes_remaining = length
        while bytes_remaining > 0 or length == 0:
            chunk = await f.read(min(COPY_BUFSIZE, bytes_remaining))
            if not chunk:
                break
            hasher.update(chunk)
            bytes_remaining -= len(chunk)
    return hasher.hexdigest()

def deref_partuuid(partuuid):
    link = '/dev/disk/by-partuuid/' + partuuid
    try:
        deref = os.readlink(link)
        dev = os.path.join(os.path.dirname(link), deref)
        return os.path.basename(dev)
    except OSError:
        return None

def get_partuuid(partition):
    dev_links = os.listdir('/dev/disk/by-partuuid')
    for link in dev_links:
        partuuid = os.path.basename(link)
        if deref_partuuid(partuuid) == partition:
            return partuuid

async def update_fstab(host, root_dev):
    # Assumption: old fstab specifies all desired partition mounts
    # Anything else the new root will have to work out for itself on boot

    async with mount(host, '/dev/' + root_dev) as mount_point:
        new_fstab_path = os.path.join(mount_point, 'etc', 'fstab')
        os.unlink(new_fstab_path)
        shutil.copyfile('/etc/fstab', new_fstab_path)
        
        new_fstab = ''
        async with aiofiles.open(new_fstab_path, 'r') as f:
            async for line in f:
                if line.strip().startswith('#') or not line.strip():
                    new_fstab += line
                else:
                    _, mnt, remainder = tuple(line.split(maxsplit=2))
                    if mnt == '/':
                        dev = get_partuuid(root_dev)
                        new_fstab += ' '.join([ f'PARTUUID={dev}', mnt, remainder ])
                    else:
                        new_fstab += line
        async with aiofiles.open(new_fstab_path, 'w') as f:
            await f.write(new_fstab)

def parse_cmdline(cmdline: str):
    return OrderedDict([ 
        tuple(kv.split('=', maxsplit=1)) if '=' in kv else (kv, '') 
        for kv in cmdline.strip().split() 
    ])

def format_cmdline(items: dict):
    return ' '.join(( 
        k if v == '' else f'{k}={v}' 
        for k, v in items.items() 
    ))

async def set_cmdline_root(cmdline_path, root_partition):
    # Update cmdline
    part_uuid = get_partuuid(root_partition)
    async with aiofiles.open(cmdline_path, 'r') as f:
        cmdline = parse_cmdline(await f.readline())
    cmdline['root'] = f'PARTUUID={part_uuid}'
    async with aiofiles.open(cmdline_path, 'w') as f:
        await f.write(format_cmdline(cmdline))

async def get_cmdline_root(cmdline_path):
    async with aiofiles.open(cmdline_path, 'r') as f:
        cmdline = parse_cmdline(await f.readline())
    root = cmdline['root']
    assert root.startswith('PARTUUID=')
    _, part_uuid = tuple(root.split('='))
    return deref_partuuid(part_uuid)


async def open_boot_config(boot_dir, read=[], write={}):
    read_items = {}
    new_config = ''
    config_txt_path = os.path.join(boot_dir, 'config.txt')
    write_needed = bool(write)
    async with aiofiles.open(config_txt_path, 'r') as f:
        async for line in f:
            k_v: List[str] = line.strip().split('=', maxsplit=1)
            if len(k_v) != 2:
                new_config += line
            else:
                k, v = k_v
                if k in read:
                    read_items[k] = v
                k = k.lstrip('#')
                if k in write:
                    new_config += f'{k}={write[k]}\n'
                    del write[k]
                else:
                    new_config += line
    if write:
        new_config += '\n'
        for k, v in write.items():
            new_config += f'{k}={write[k]}\n'
    if write_needed:
        async with aiofiles.open(config_txt_path, 'w') as f:
            await f.write(new_config)
    return read_items


async def move_firmware_files(prefix_dir, boot_dir):
    """Move anything not "OS" (kernels, initramfs, cmdline.txt, .dtbs and overlays)"""
    not_os_files = [ 
        f for f in glob(os.path.join(prefix_dir, '**', '**'), recursive=True)
        if not RE_OS_FILES.search(f) and os.path.isfile(f)
    ]
    for f in not_os_files:
        rel_path = os.path.relpath(f, prefix_dir)
        dest = os.path.join(boot_dir, rel_path)
        dir_name = os.path.dirname(dest)
        makedirs(dir_name, exist_ok=True)
        os.rename(f, dest)
        await asyncio.sleep(0)


async def copy_os_files(source_dir, dest_dir):
    os_files = [
        f for f in glob(os.path.join(source_dir, '**', '**'), recursive=True)
        if os.path.isfile(f) and RE_OS_FILES.search(f)
    ]
    for f in os_files:
        rel_path = os.path.relpath(f, source_dir)
        dest = os.path.join(dest_dir, rel_path)
        dir_name = os.path.dirname(dest)
        makedirs(dir_name, exist_ok=True)
        shutil.copyfile(f, dest)
        await asyncio.sleep(0)


async def hashsum_check(hash_name, dir, sha256_file):
    try:
        async with aiofiles.open(sha256_file) as f:
            async for line in f:
                digest, path = tuple(line.strip().split())
                if not re.match(r'[0-9A-Za-z]+', digest):
                    die(f'Value {digest} is not a hash. Expecting hex.')                
                actual_digest = await digest_file(hashlib.new(hash_name), os.path.join(dir, path))
                if digest.lower() != actual_digest.lower():
                    die(f'File {path} hash verification failed')
    except FileNotFoundError:
        die('One of the required files is missing')

HASH_URL_DEFAULT = '<OS_IMAGE_URL>.replace(.gz, .<hash_type>)'
BOOT_HASH_URL_DEFAULT = '<boot_files_url>.replace(.tar.gz, <hash_type>)'

@app.command(context_settings={ 'auto_envvar_prefix': 'AB' })
@run_sync
async def main(
    boot_files_url: str = typer.Option(None, help='URL to tar/gzipped boot files (or skip boot files update if not set).'),
    boot_files_hash_url: str = typer.Option(BOOT_HASH_URL_DEFAULT, help='URL to hash file for boot files.'),
    hash_url: str = typer.Option(HASH_URL_DEFAULT, help='URL to a file containing a hash, used to verify the write.'),
    hash_type: str = typer.Option('sha256', help='Type of hash contained in the hash file'),
    host: str = typer.Option('/', help='Path to host\'s active root (usually only needed for Docker)'),
    version_file: str = typer.Option('/.ab_version', help='File to check for existing version (at host)'),
    reboot: bool = typer.Option(True, help='Reboot after an update.'),
    boot_partition = typer.Option(1, help='Partition number containing boot files'),
    os_partitions: Tuple[int, int] = typer.Option((2, 3), help='Which two OS partition numbers to consider for flashing'),
    force: bool = typer.Option(False, help='Assume upgrade is required.'),
    dry_run: bool = typer.Option(False, help='Take no actions.'),
    verbose: int = typer.Option(0, '-v', count=True, help='Verbosity of logging. Specify multiple times for more.'),
    os_image_url: str = typer.Argument(..., envvar='AB_OS_IMAGE_URL', help='URL to new gzipped image.'),
    new_version: str = typer.Argument(None, envvar='AB_NEW_VERSION', help='New version number (e.g. 1.2.3).'),
):
    log_level = max(logging.DEBUG, logging.WARNING - (logging.DEBUG * verbose))
    logging.basicConfig(level=log_level)
    log.debug(f'Debug logging is active')

    if not force and not new_version:
        log.critical('You must specify a NEW_VERSION unless you --force.')
        return 1

    if force:
        log.warning('Skipping version check due to "force" flag')
    elif not await check_version_file(host, version_file, new_version, dry_run):
        log.info('No upgrade is required')
        return

    os_partitions = sorted(os_partitions)
    active_part = await get_device_for_path(host)
    disk_dev = await get_parent_device(active_part)
    disk_parts = await list_partitions(disk_dev)
    try:
        allowed_partitions = [ disk_parts[p - 1] for p in os_partitions ]
    except IndexError:
        die(f'Couldn\'t find OS partitions. Review the value for --os-partitions.')
    log.info(f'Active partition is {active_part}')
    if not active_part in allowed_partitions:
        die(f'Active partition ({active_part}) is not in the provided list ({",".join(allowed_partitions)}).')
    boot_dev = disk_parts[boot_partition - 1]

    letters = 'ab'
    boot_prefix = 'a'
    for n, p in enumerate(allowed_partitions):
        if p != active_part:
            boot_prefix = letters[n]
            inactive_part = p
    part_name, part_size = await get_partition(host, inactive_part)
    if not part_name:
        die(f'Proposed inactive partition ({inactive_part}) doesn\'t exist')
    log.info(f'{part_name} will be flashed.')
    part_size = int(part_size)
    if dry_run:
        log.warning('Dry run - will write to /dev/null')
        part_name = 'null'
        part_size = sys.maxsize

    partition_to_flash = '/dev/' + part_name
    max_flash_bytes = part_size

    async def write_os():
        nonlocal hash_url

        bytes_written = 0
        async def count_bytes(send):
            nonlocal bytes_written
            while True:
                chunk = yield
                bytes_written += len(chunk)
                await send(chunk)

        write_task = asyncio.create_task(
            pipeline(
                lambda n: get_http_bytes(os_image_url, n),
                decompress,
                count_bytes,
                lambda: write_to_partition(partition_to_flash, max_flash_bytes)
            )
        )
        
        default_hash_url = hash_url == HASH_URL_DEFAULT
        if default_hash_url:
            hash_url = os_image_url.replace('.gz', f'.{hash_type}')
        hasher = hashlib.new(hash_type)
        try:
            async with aiohttp.request('GET', hash_url, raise_for_status=True) as resp:
                expected_hash = (await resp.content.read(hasher.digest_size * 2)).decode()
            if not re.match(r'[0-9A-Za-z]+', expected_hash):
                die(f'Value fetched from {hash_url} is not a hash. Expecting hex.')
            await write_task
            log.info(f'Parition write done, starting verification (expecting {expected_hash})')
            actual_hash = await digest_file(hasher, partition_to_flash, bytes_written)
            if expected_hash != actual_hash:
                die(f'Written OS data does not match fetched hash ({actual_hash})')
            log.info('Write verification completed successfully')
        except ClientResponseError:
            if default_hash_url:
                log.warning('Skipping write verification as no hash file could be fetched')
                await write_task
            else:
                raise

        log.debug('Expanding written filesystem')
        await run('e2fsck', '-fp', partition_to_flash)
        await run('resize2fs', partition_to_flash)
        await update_fstab(host, part_name)

    async def write_boot():
        nonlocal boot_files_hash_url

        # TODO smarter partition detection
        async with mount(host, boot_dev) as mountpoint:
            INACTIVE_BOOT = os.path.join(mountpoint, boot_prefix)
            rmtree(INACTIVE_BOOT, ignore_errors=True)
            os.makedirs(INACTIVE_BOOT, exist_ok=True)
            if boot_files_url:
                try:
                    await pipeline(
                        lambda n: get_http_bytes(boot_files_url, n),
                        decompress,
                        lambda: untar(os.path.join(mountpoint, INACTIVE_BOOT))
                    )
                except:
                    rmtree(INACTIVE_BOOT)
                    raise
                log.info('Wrote boot files')

                default_boot_hash = boot_files_hash_url == BOOT_HASH_URL_DEFAULT
                if default_boot_hash:
                    boot_files_hash_url = boot_files_url.replace('.tar.gz', '.' + hash_type)
                tmpdir = mkdtemp()
                hash_file = os.path.join(tmpdir, 'hashes')
                try:
                    try:
                        async with aiofiles.open(hash_file, 'wb') as f:
                            await get_http_bytes(boot_files_hash_url, f.write)
                        log.info('Verifying boot files')
                        await hashsum_check(hash_type, INACTIVE_BOOT, hash_file)
                    except ClientResponseError:
                        if not default_boot_hash:
                            die('Could not fetch specified boot_files_hash_url')
                finally:
                    rmtree(tmpdir)
            else:
                log.warning('No boot files specified, skipping download')
                # Build from existing boot files - from root plus current active
                boot_config = await open_boot_config(mountpoint, ['os_prefix'])
                active_boot = boot_config.get('os_prefix', '').rstrip('/')
                if os.path.isdir(active_boot):
                    log.info('os_prefix is currently configured - will copy this for use')
                    rmtree(INACTIVE_BOOT)
                    shutil.copytree(active_boot, INACTIVE_BOOT)
                else:
                    await copy_os_files(mountpoint, INACTIVE_BOOT)

        await set_cmdline_root(os.path.join(INACTIVE_BOOT, 'cmdline.txt'), part_name)

    for t in asyncio.as_completed([write_os(), write_boot()]):
        await t
    
    log.info('All was successful. Configuring boot loader.')

    # The point of no return
    async with mount(host, boot_dev) as mountpoint:
        await move_firmware_files(boot_prefix, mountpoint)
        await open_boot_config(mountpoint, write={'os_prefix': boot_prefix + '/'})

    if reboot:
        log.info('Rebooting')
        await run('reboot')

if __name__ == '__main__':
    app()
